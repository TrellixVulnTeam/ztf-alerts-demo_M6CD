{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZTF alerts demo\n",
    "\n",
    "In this demo, we will:\n",
    "- Set up `Docker`\n",
    "- Using `Docker`, deploy a `MongoDB` database, fetch a night worth of (pre-filtered to reduce size) public ZTF alerts, ingest it into the database, and create indices in the database for faster queries\n",
    "- Set up `Robo3T` and use it to look at the database and query it\n",
    "- Query the database using `python`\n",
    "- Inspect the contents of an alert packet\n",
    "- Construct and plot a light curve\n",
    "- Plot the cutout images from an alert packet\n",
    "\n",
    "\n",
    "### Download and install `Docker` and `Robo3T`\n",
    "- Download and install the appropriate version of `Docker` for your platform from [here](https://www.docker.com/community-edition). You will need to create an account on their website.\n",
    "- Download and install `Robo3T` from [here](https://robomongo.org/download).\n",
    "  We will use it to connect to the database.\n",
    "\n",
    "### Fetch, build, and run the code to deploy a `MongoDB` database, fetch a night worth of (pre-filtered to reduce size) public ZTF alerts, ingest it into the database, and create indices in the database for faster queries\n",
    "\n",
    "This is a lot of stuff! Sounds scary, however with the help of `Docker`, we will only have to run a few simple commands to do all that.\n",
    "\n",
    "Clone the repo and `cd` into the directory:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/dmitryduev/ztf-alerts-demo.git\n",
    "cd ztf-alerts-demo\n",
    "```\n",
    "\n",
    "The inside/contents of a `Docker` container get destroyed when it is removed, so we need to tell `Docker` to keep the useful data in a \"persistent\" storage.\n",
    "\n",
    "Create a so-called persistent `Docker` volume for `MongoDB`:\n",
    "```bash\n",
    "docker volume create alert-fetcher-mongo-volume\n",
    "```\n",
    "\n",
    "Launch the `MongoDB` container. (Feel free to change u/p for the db admin)\n",
    "```bash\n",
    "docker run -d --restart always --name alert-fetcher-mongo -p 27018:27017 \\\n",
    "       -v alert-fetcher-mongo-volume:/data/db \\\n",
    "       -e MONGO_INITDB_ROOT_USERNAME=mongoadmin \\\n",
    "       -e MONGO_INITDB_ROOT_PASSWORD=mongoadminsecret \\\n",
    "       mongo:latest\n",
    "```\n",
    "\n",
    "That's it for the database set-up! You can now connect to it on `localhost` on port `27018`.\n",
    "\n",
    "Finally, build and launch the alert-fetcher container. We will bind-mount a directory on your host machine to store the alerts:\n",
    "```bash\n",
    "cd alert-fetcher\n",
    "docker build -t alert-fetcher -f Dockerfile .\n",
    "# make sure path ./alerts (or whatever path you specify) exists\n",
    "docker run -v ./alerts:/alerts \\\n",
    "           --name alert-fetcher -d --link alert-fetcher-mongo:mongo -it alert-fetcher\n",
    "```\n",
    "\n",
    "This will launch a program that will fetch a pre-filtered (demo) set of public ZTF alerts from July 13, 2018, ingest that into the MongoDB database, and create indicies to accelerate queries. The script will add a `coordinates` field to each that is not part of the original alert packets, to allow 2d indexation on the sphere for fast positional/cone searches. Additionally, a unique string `candid_objectId` is used a the alert identifier in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can now use `Robo3T` to connect to the database and take a look at the alerts. Launch `Robo3T` and click \"create\" connection. \n",
    "- Name the connection, for example, `local_docker_alerts_demo`\n",
    "- Use `localhost` : `27018` as the address\n",
    "- On the `Authentication` tab, check \"perform authentication\", and use `admin` for `Database`, `mongoadmin` for `User Name`, `mongoadminsecret` for 'Password'(if you did not choose other u/p), and `SCRAM-SHA-1` for `Auth Mechanism`\n",
    "- Click `Save` and then `Connect`\n",
    "\n",
    "`Robo3T` will connect to the database. To see the ingested alerts, click `ztf_alerts` -> `Collections` -> `alerts`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Notes [optional]\n",
    "\n",
    "To check the running/stopped containers, type:\n",
    "```bash\n",
    "docker ps -a\n",
    "```\n",
    "\n",
    "To stop and remove a container, run:\n",
    "```bash\n",
    "docker stop container_name\n",
    "docker rm -f container_name\n",
    "```\n",
    "\n",
    "To get all public alerts from the ZTF archive for a given night, remove `--demo` from the last line of the file `alert-fetcher/Dockerfile` and change the date string. Then re-build and re-run the alert-fetcher container.\n",
    "\n",
    "Every time when you change the code that is used inside a container, the latter must be re-build and restarted.\n",
    "\n",
    "Alerts are stored in the database in a serialized binary format that resembles the Apache Avro format used in the packets in that it can be easily converted into `json` or a `python` dictionary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore the contents of an avro packet. We will also learn how to construct and display a light curve for an alert and plot the cutout images.\n",
    "\n",
    "_NOTE:_ You may want to look at Erik Bellm's Jupyter notebook with some more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import fastavro\n",
    "\n",
    "from astropy.time import Time\n",
    "from astropy.io import fits\n",
    "import aplpy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a particular avro packet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_packet = './alerts/20180601/516170410015015005.avro'\n",
    "with open(avro_packet, 'rb') as fa:\n",
    "    freader = fastavro.reader(fa)\n",
    "    schema = freader.schema\n",
    "\n",
    "    for packet in freader:\n",
    "        print(packet.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema is stored in the packet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payload, once in memory, is a python dictionary, so the attributes are easy to access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('JD: {} Filter: {} Mag: {:.2f}+/-{:.2f}'.format(\n",
    "    packet['candidate']['jd'],packet['candidate']['fid'],\n",
    "    packet['candidate']['magpsf'],packet['candidate']['sigmapsf']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE ESPECIALLY: the magnitudes here do not include the magnitude of the underlying reference source (if present), so if this is a variable star further adjustment is needed. Example to come...\n",
    "\n",
    "Record access like this is a little verbose; let's wrap things up in a dataframe for ease of access (and faster loading).\n",
    "\n",
    "Now let's extract the lightcurves. The alert packet formats are nested, so the historical detections (if present) have the same structure as the candidate triggering the alert (minus a couple fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(packet):\n",
    "    df = pd.DataFrame(packet['candidate'], index=[0])\n",
    "    df_prv = pd.DataFrame(packet['prv_candidates'])\n",
    "    return pd.concat([df, df_prv], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflc = make_dataframe(packet)\n",
    "dflc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some of the historical detections are upper limits, signified by the NaNs. Note that the most recent candidate has a few fields that are not present for the prv_candidates.\n",
    "\n",
    "Let's plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lightcurve(dflc, days_ago=True):\n",
    "    \n",
    "    filter_color = {1: 'green', 2: 'red', 3: 'pink'}\n",
    "    if days_ago:\n",
    "        now = Time.now().jd\n",
    "        t = dflc.jd - now\n",
    "        xlabel = 'Days Ago'\n",
    "    else:\n",
    "        t = dflc.jd\n",
    "        xlabel = 'Time (JD)'\n",
    "    \n",
    "    plt.figure()\n",
    "    for fid, color in filter_color.items():\n",
    "        # plot detections in this filter:\n",
    "        w = (dflc.fid == fid) & ~dflc.magpsf.isnull()\n",
    "        if np.sum(w):\n",
    "            plt.errorbar(t[w], dflc.loc[w, 'magpsf'], dflc.loc[w, 'sigmapsf'],\n",
    "                         fmt='.', color=color)\n",
    "        wnodet = (dflc.fid == fid) & dflc.magpsf.isnull()\n",
    "        if np.sum(wnodet):\n",
    "            plt.scatter(t[wnodet], dflc.loc[wnodet, 'diffmaglim'], \n",
    "                        marker='v', color=color, alpha=0.25)\n",
    "    \n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lightcurve(dflc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's figure out how to display the cutout images. These are gzip-compressed fits files stored as bytes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
